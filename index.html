<html lang="en">

<head>
<title>Audio Samples</title>
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="theme-color" content="#ffffff">
<style>
  .logo{
    margin-left: 17%;
    float: left;
  }
  
  .banner{
    color: #3265C2;
    margin-left: 2%;
    float: left;
  }

  .closer { 
    line-height: 10px;
  }

  .row:after {
    content: "";
    display: flex;
    clear: both;
  }

  .case{
    font-weight: bold;
    text-transform: capitalize;
  }

  body {
    margin: 20;
    font-family: Arial, Helvetica, sans-serif;
  }

  #topBtn {
    display: none;
    position: fixed;
    bottom: 20px;
    right: 30px;
    z-index: 99;
    font-size: 18px;
    border: none;
    outline: none;
    background-color: #555;
    color: white;
    cursor: pointer;
    padding: 15px;
    border-radius: 4px;
  }

  #topBtn:hover {
    background-color: #3265C2;
  }
  
  table {
    border-collapse: collapse;
    margin-top: 10px; 
    margin-bottom: 20px;
  }
  
  th {
    border-style : hidden;
    text-align: center;
  	background-color: #3265C2;
  	color: white;
  	height: 50px;
    font-size: 140%
  }

  td {
    text-align: left;
    padding-bottom: 5px;
  }

  .no-border {
    border-style : hidden!important;
    padding-top: 5px;
    padding-bottom: 5px;
    text-align: justify;
  }

  .no-topborder {
    border-top-color: #FFFFFF!important;
  }

  .sigInfo{
    width: 20%;
    padding-top: 15px;
  }

  .setup{
    display: block;
    margin-left: auto;
    margin-right: auto;
    width: 70%;
  }

  .text-in-table{
    text-align: justify;
    padding-top: 5px;
    padding-bottom: 5px;
  }

  .sigTag{
    text-align: center;
    font-weight: bold;
    padding-top: 25px;
  }

  .noiseInfo{
/*    text-align: center;*/
/*    color: #3265C2;*/
    height: 40px;
    font-size: 120%;
    font-weight: bold;
  }

  .spectrogram{
    width: 100%;
  }

  .audio{
    display: block;
    margin-left: auto;
    margin-right: auto;
  }
  
  tr {
    border-bottom: 2px solid black;
  }
  
  .original {
    transform: scale(1);
    position: inherit;
    z-index: 0;
  }
  
  .enlarged {
    transform: scale(2);
    position: relative;
    z-index: 100;
  }

  .reference{
    font-family: 'Times New Roman';
    font-size: 12;
  }

  /*for navigation bar*/
  .navi-bar {
    list-style-type: none;
    margin: 0;
    padding: 0;
    width: 15%;
    background-color: #f1f1f1;
    position: fixed;
    height: 100%;
    overflow: auto;
  }

  .navi-item {
    display: block;
    color: #000;
    padding: 8px 16px;
    text-decoration: none;
  }

  .active {
    background-color: #3265C2;
    color: white;
  }

  li a:hover:not(.active) {
    background-color: #555;
    color: white;
  }

  .content{
   margin-left: 17%;
  }

  p.content-text{
   margin-left: 17%;
   text-align: justify;
  }


</style>
</head>

<body>

<div>

  <ul class='navi-bar'>
    <li><a class="navi-item active" href="#home">Home</a></li>
    <li><a class="navi-item location" href="#location">Location-informed DNN for source separation</a></li>
    <li><a class="navi-item" href="#binaural">Proof of concept for binaural augmentation</a></li>
    <li><a class="navi-item" href="#about">About ASPIRE</a></li>
  </ul>

</div>

<button onclick="topFunction()" id="topBtn" title="Go to top">Top</button>

<div id='home'>
   <div class="row">
    <div class="logo"><img src=".\img\logo_ugent_nl.svg"></div>
    <div class="banner">
      <h1 class="closer">ASPIRE</h1>
      <p class="closer" style="font-size: 90%;">Audio and Signal Processing, Interpretation, and Enhancement@IDLAB</p>
    </div>
  </div>

</div>

<h3 class="content">Audio samples </h3>

<p class="content-text">
  These pages present demos of our latest work in deep source separation. In one of the first approaches of its kind, we present <em>location-informed source extraction</em> where the network can be dynamically configured to focus on <em>any</em> desired spatial region. <br/>
  At run-time, information on such desired spatial regions (analogous to a desired beamwidth) can be input to the system (e.g., by an external source localisation stage or by the user). Our network then extracts <em>all speech sources</em> in the specified region.  The regions of interest can, additionally, be <em>reconfigured during run-time</em>.
</p>

<div class="content" id="location">
	
  <table class="tb">
    <thead>
      <tr>
        <th colspan="3">Speaker extraction using compact microphone arrays</th>
      </tr>
    </thead>

    <tbody>
      <tr>
        <td class="no-border" width="60%" colspan="2">
          We first present the separation results obtained using a small 3-microphone array. In this scenario, the speech of 2 talkers is captured in a “cocktail-party” scenario where the background noise is spatially diffuse babble noise (noise produced by several people holding simultaneous conversations – typical of a party). The two potential target speakers are at a distance of 2m from the array (see Figure).
        </td>
        <td class="no-topborder" rowspan="2">
          <p class="sigTag">
            Microphone observation
          </p>
          <p align="center">First, let us listen to the captured signal at any one microphone</p>
          <img src=".\img\mic.png" class="spectrogram"><br/>
          <audio controls="" class="audio"><source src="./audio/mic_re.wav"></audio>
        </td>
      </tr>

      <tr>
        <td class="sigInfo">
          Setup: 
          <ul>
            <li>Compact microphone array (M=3)</li>
            <li>in a reverberant room (T<sub>60</sub>=0.6s)</li>
          </ul>
        </td>
        <td>
          <img src=".\img\separation_setup.png" class=setup><br/>
        </td>
      </tr>

      <tr>
        <td class="text-in-table" colspan="3">
          Each speaker is considered a target in turn and 2 outputs are generated, one corresponding to each speaker. Location information can be included in different ways – either as an <em>input feature</em> or as an <em>attention mechanism</em> deeper within the network. We show the effect of both these choices in the examples below.
        </td>
      </tr>

      <tr>
        <td class="sigInfo">
           Now, let us listen to the output of location-informed separation, when the location information is incorporated as an <em>input feature</em>. This is the most straightforward way to include additional information, but by introducing it this early, the DNN may not be able to make the most optimal use of the information.
        </td>
        <td>
          <p class="sigTag">
            Output-Female
          </p>
          <img src=".\img\single_LBI_HC_ch0.png" class="spectrogram"><br/>
          <audio controls="" class="audio"><source src="./audio/single_LBI_HC_ch0.wav"></audio>
        </td>
        <td>
          <p class="sigTag">
            Output-Male
          </p>
          <img src=".\img\single_LBI_HC_ch1.png" class="spectrogram"><br/>
          <audio controls="" class="audio"><source src="./audio/single_LBI_HC_ch1.wav"></audio>
        </td>
      </tr>

      <tr>
        <td class="sigInfo">
           Now we generate location sensitivity <em>deeper</em> in the network, allowing the DNN to optimally represent and use this information for the separation. This yields much better separation and clearer representation of the two target signals. 
        </td>
        <td>
          <p class="sigTag">
            Output-Female
          </p>
          <img src=".\img\single_LDE_HC_ch0.png" class="spectrogram"><br/>
          <audio controls="" class="audio"><source src="./audio/single_LDE_HC_ch0.wav"></audio>
        </td>
        <td>
          <p class="sigTag">
            Output-Male
          </p>
          <img src=".\img\single_LDE_HC_ch1.png" class="spectrogram"><br/>
          <audio controls="" class="audio"><source src="./audio/single_LDE_HC_ch1.wav"></audio>
        </td>
      </tr>

      <tr>
        <td class="sigInfo">
           Lastly, let us see the further benefit of cleaning up the phase in the separated signals. This is done by an additional <em>deep-cascade</em> architecture, with skip connections to preserve the information from the earlier layers. 
        </td>
        <td>
          <p class="sigTag">
            Output-Female
          </p>
          <img src=".\img\cascade_LDE_HC_ch0.png" class="spectrogram"><br/>
          <audio controls="" class="audio"><source src="./audio/cascade_LDE_HC_ch0.wav"></audio>
        </td>
        <td>
          <p class="sigTag">
            Output-Male
          </p>
          <img src=".\img\cascade_LDE_HC_ch1.png" class="spectrogram"><br/>
          <audio controls="" class="audio"><source src="./audio/cascade_LDE_HC_ch1.wav"></audio>
        </td>
      </tr>

      <tr>
        <td class="sigInfo">
          The clean speech signals (this serves as <em>reference</em> to estimate the quality of the separation)
        </td>
        <td>
          <p class="sigTag">
            Clean-Female
          </p>
          <img src=".\img\s_dry_ch0.png" class="spectrogram"><br/>
          <audio controls="" class="audio"><source src="./audio/s_dry_ch0.wav"></audio>
        </td>
        <td>
          <p class="sigTag">
            Clean-Male
          </p>
          <img src=".\img\s_dry_ch1.png" class="spectrogram"><br/>
          <audio controls="" class="audio"><source src="./audio/s_dry_ch1.wav"></audio>
        </td>
      </tr>


    </tbody>
	</table>
</div>

<div class="content" id="binaural">
  
  <table class="tb">
    <thead>
      <tr>
        <th colspan="3">Proof of concept for binaural augmentation</th>
      </tr>
    </thead>

    <tbody>
      <tr>
        <td class="sigInfo" colspan="3">
          Now we show a <em>proof-of-concept</em> work for binaural hearing augmentation – work we have <em>recently started</em>. The results here are based on the <a href="http://claritychallenge.org">Clarity challenge</a> data, where the user wears <em>binaural</em> behind-the-ear (BTE) hearing-aids, each equipped with 3 microphones. The goal is to extract the signal of the target speaker in the presence of localised interference which could either be a household appliance (boiling kettle, vacuum cleaner, …) or an interfering talker. The separation is based on a classical beamformer-postfilter combination, where the <em>statistic estimation</em> is driven by deep learning. Example scenarios are demonstrated below, with:
          <ul>
            <li>the left panel showing the noisy mixture signal at one ear and </li>
            <li>the right panel showing the right channel of the binaural target signal estimate.</li>
          </ul>
          We have focussed on fully attenuating the interfering source – which is an acceptable strategy because of the <em>domestic</em> setting. <br>
          In future work, for generalised augmentation, we will consider location and <em>scene informed</em> source separation – which will yield the appropriate context dependent enhancement. 
        </td>
      </tr>

      <tr>
        <td class="noiseInfo" colspan="3">
            Speech in domestic noise
        </td>
      </tr>


      <tr>
        <td class="sigInfo">
          Scene 1: Running water
        </td>
        <td>
          <p class="sigTag">
            Noisy observation
          </p>
          <img src=".\img\S06253-beamforming-mixed.png" class="spectrogram"><br/>
          <audio controls="" class="audio"><source src="./audio/S06253-beamforming-mixed.wav"></audio>
        </td>
        <td>
          <p class="sigTag">
            Enhanced binaural signal
          </p>
          <img src=".\img\S06253-beamforming-suppressed_ch1.png" class="spectrogram"><br/>
          <audio controls="" class="audio"><source src="./audio/S06253-beamforming-suppressed_ch1.wav"></audio>
        </td>
      </tr>

      <tr>
        <td class="sigInfo">
          Scene 2: Vacuum cleaner
        </td>
        <td>
          <p class="sigTag">
            Noisy observation
          </p>
          <img src=".\img\S07845-beamforming-mixed.png" class="spectrogram"><br/>
          <audio controls="" class="audio"><source src="./audio/S07845-beamforming-mixed.wav"></audio>
        </td>
        <td>
          <p class="sigTag">
            Enhanced binaural signal
          </p>
          <img src=".\img\S07845-beamforming-suppressed_ch1.png" class="spectrogram"><br/>
          <audio controls="" class="audio"><source src="./audio/S07845-beamforming-suppressed_ch1.wav"></audio>
        </td>
      </tr>

      <tr>
        <td class="noiseInfo" colspan="3">
            Speech in speech (target: the 2nd speaker)
        </td>
      </tr>

      <tr>
        <td class="sigInfo">
          Clip A
        </td>
        <td>
          <p class="sigTag">
            Noisy observation
          </p>
          <img src=".\img\S07835-beamforming-mixed.png" class="spectrogram"><br/>
          <audio controls="" class="audio"><source src="./audio/S07835-beamforming-mixed.wav"></audio>
        </td>
        <td>
          <p class="sigTag">
            Enhanced binaural signal
          </p>
          <img src=".\img\S07835-beamforming-suppressed_ch1.png" class="spectrogram"><br/>
          <audio controls="" class="audio"><source src="./audio/S07835-beamforming-suppressed_ch1.wav"></audio>
        </td>
      </tr>

      <tr>
        <td class="sigInfo">
          Clip B
        </td>
        <td>
          <p class="sigTag">
            Noisy observation
          </p>
          <img src=".\img\S07837-beamforming-mixed.png" class="spectrogram"><br/>
          <audio controls="" class="audio"><source src="./audio/S07837-beamforming-mixed.wav"></audio>
        </td>
        <td>
          <p class="sigTag">
            Enhanced binaural signal
          </p>
          <img src=".\img\S07837-beamforming-suppressed_ch1.png" class="spectrogram"><br/>
          <audio controls="" class="audio"><source src="./audio/S07837-beamforming-suppressed_ch1.wav"></audio>
        </td>
      </tr>


    </tbody>
  </table>
</div>

<div class="content" id="about">
  <p>
    <h4>About ASPIRE:</h4>
    ASPIRE@IDLab is the research group headed by Prof. Dr.-Ing Nilesh Madhu at Ghent University. The research focus of the group on the robust capture, enhancement, and analysis of signals, for information extraction in a wide variety of applications. Audio and speech processing is one of our main research pillars, and our ongoing research here is focussed on: <br>
    (a) localising and segregating speech sources for telecommunications and smart-home applications; <br>
    (b) audio analytics: classifying the background scene, localising acoustic events, ...;<br>
    (c) audio reproduction for group experience in virtual reality.<br>

    The underlying philosophy in our approaches is the <em>appropriate</em> embedding of <em>domain-specific</em> knowledge within <em>deep-learning</em> frameworks. This can better leverage the available data, while allowing the networks to generalise well to unseen conditions in real-life situations. <br>

    With our latest work, we are at the forefront in the field of single- and multi-microphone speech enhancement.
  </p>
  <p>
    <h4>Other interesting demos by ASPIRE: </h4>
    <ul>
      <li><a href="https://ieeexplore.ieee.org/document/9829294">TASLP</a>: <a href="https://yanjuesong.github.io/Improved-CEM-samples/">Improved CEM for Speech Harmonic Enhancement in Single Channel Noise Suppression</a> </li>
      <li><a href="https://ieeexplore.ieee.org/document/10096868/">ICASSP 2023</a>: <a href="https://yanjuesong.github.io/CEM-loss-samples/">Aiding speech harmonic recovery in DNN-based single channel noise reduction using cepstral excitation manipulation (CEM) components</a> </li>
      <li><a href="https://ieeexplore.ieee.org/document/10094862">ICASSP 2023</a>: <a href="https://users.ugent.be/~sbkindt/ICASSP2023/">Exploiting speaker embeddings for improved microphone clustering and speech separation in ad-hoc microphone arrays</a> </li>
      <li><a href="https://aspireugent.github.io/Selective-Speaker-Separation/">A DOA Dependent Feature Extraction for Selective Speaker Separation With a U-Net Architecture</a> </li>
    </ul>
  </p>

  <p>
    <!-- <img alt="Hits" src="https://hits.sh/aspireugent.github/SOTA-demos.svg?color=3265C2&labelColor=ffffff"/> -->
    <img src="https://hits.sh/aspireugent.github.io/SOTA-demos.svg"
      style="float: right;padding-right: 15px;" 
      width="5" />
  </p>

</div>


<br/>

<script>

// Top button
//Get the button
var mybutton = document.getElementById("topBtn");

// When the user scrolls down 20px from the top of the document, show the button
window.onscroll = function() {scrollFunction()};

function scrollFunction() {
  if (document.body.scrollTop > 20 || document.documentElement.scrollTop > 20) {
    mybutton.style.display = "block";
  } else {
    mybutton.style.display = "none";
  }
}

// When the user clicks on the button, scroll to the top of the document
function topFunction() {
  document.body.scrollTop = 0;
  document.documentElement.scrollTop = 0;
}


</script>

</body>
